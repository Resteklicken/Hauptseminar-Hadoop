\chapter{Hadoop Grundlagen}
\textit{``Die Apache Hadoop Softwarebibliothek ist ein Framework, das die über Computercluster verteilte Verarbeitung großer Datensätze mit einfachen Programmiermodellen ermöglicht. Es ist so konzipiert, dass es von einzelnen Servern bis hin zu Tausenden von Rechnern skaliert werden kann, von denen jeder lokale Rechenleistung und Speicherplatz bietet. Anstatt sich auf Hardware zu verlassen, um eine hohe Verfügbarkeit zu gewährleisten, ist die Bibliothek selbst so konzipiert, dass sie Ausfälle auf der Anwendungsebene erkennt und bewältigt, so dass ein hochverfügbarer Dienst auf einem Cluster von Computern bereitgestellt wird, von denen jeder für sich für Ausfälle anfällig sein kann.''}\cite{noauthor_apache_nodate}
\par
So beschreibt (übersetzt aus dem Englischen) die Apache Software Foundation ihr Top Level Projekt \textbf{Apache Hadoop}. Diese Arbeit wird einen pragmatischen Überblick über Hadoop und die Komponenten im Hadoop Ecosystem geben. Dabei soll der Fokus nicht auf technischen Details, sondern auf dem praktischen Einsatz von Hadoop liegen. Es soll anhand von Anwendungsfällen demonstriert werden, wie die einzelnen Hadoop Komponenten zur Lösung bestimmter Problemstellungen auf- und eingesetzt werden.

\section{Technischer und geschichtlicher Hintergrund}
\subsection{Anforderungen von Big Data}
``Der Begriff „Big Data“ bezieht sich auf Datenbestände, die so groß, schnelllebig oder komplex sind, dass sie sich mit herkömmlichen Methoden nicht oder nur schwer verarbeiten lassen.''\cite{noauthor_big_nodate}
\par 
Schon Anfang der Neunziger war es nicht mehr praktikabel, Webseiten händisch, zum Beispiel in ''Web Directories'', zu katalogisieren. Man wollte Nutzern trotzdem die Möglichkeit geben, Informationen durch das Durchsuchen zentraler Anlaufstellen ausfindig zu machen. Automatisierte Tools, die sogenannten ''Web Crawler'' wurden erfunden, um diese Arbeit zu übernehmen.\cite{griffiths_search_2007}
\par 
Das Internet erlebte in den letzten Jahren des 20. Jahrhunderts ein explosionsartiges Wachstum an Nutzern und Webseiten, und damit auch an Informationen, die katalogisiert werden mussten.\cite{zakon_hobbes_2018} 
Um eine immer größer werdende Menge an Informationen verarbeiten zu können, gibt es zwei Ansätze der Skalierung: Vertikale und horizontale Skalierung. Diese sollen in den folgenden Abschnitten erläutert werden, um die Designphilosophie hinter Hadoop zu verstehen.

\subsection{Vertikale Skalierung}
Bei der vertikalen Skalierung (''scaling up'') werden \textit{einem} System mehr Ressourcen wie zum Beispiel größerer Speicher, oder eine schnellere CPU hinzugefügt. Dadurch bekommt man einen Performance-Gewinn: Man kann mehr Daten speichern, oder Berechnungen werden schneller fertig gestellt.
Ein großer Vorteil der vertikalen Skalierung ist, dass Anwendungsprogramme in der Regel nicht angepasst werden müssen, um vom diesem Performance-Wachstum zu profitieren. Wenn man eine 5TB große Festplatte gegen eine 10TB Festplatte austauscht, dann hat man den Speicherplatz eines Servers vertikal skaliert. Die darauf laufenden Programme müssen nicht angepasst werden, sondern man kann einfach doppelt so viele Daten speichern.\cite{beaumont_how_2014}
\par
Vertikale Skalierung hat drei große Nachteile: Erstens kann man nicht unbegrenzt vertikal skalieren. Ein Server kann physisch nur eine begrenzte Anzahl an Hardware aufnehmen. Zweitens wächst die Performance eines Systems bei vertikaler Skalierung höchstens linear\cite{gustafson_amdahls_2011}, die Kosten allerdings nicht\cite{noauthor_horizontal_nodate}. Heutzutage kann man gerade bei Cloud-Anbietern sehr leistungsfähige Systeme bei linearem Preisanstieg mieten.\cite{noauthor_pricing_nodate} Sucht man aber noch mehr Performance in \textit{einem} System, dann steigen die Kosten exponentiell\cite{athow_at_2020}. Drittens skalieren nicht alle Faktoren in einem System gleich gut vertikal. Die Speicherkapazität von SSDs ist zum Beispiel seit 1978 von 45MB auf 100TB gestiegen (Faktor $2222,22 \cdot 10^{3}$), während sich die Datenrate nur von 1.5MB/s auf 500/460MB/s (Sequential Read/Write) erhöht hat (Faktor $0,333 \cdot 10^{3}$).\cite{athow_at_2020}\cite{noauthor_who_nodate}  

\subsection{Horizontale Skalierung}
Ein Cluster ist ein Verbund aus Computern (Nodes), die wie ein einziger, deutlich leistungsfähigerer Computer arbeiten. Aufgaben und Daten werden in kleinere Teile zerlegt und auf alle Nodes im Cluster aufgeteilt, welche dann parallel Teilaufgaben lösen. Ergebnisse werden zusammengefügt und zurückgegeben. Anders als bei der vertikalen Skalierung kann man gerade in Zeiten des Cloud Computings praktisch unendlich horizontal skalieren. Allerdings muss man dafür kompliziertere Anwendungslogik verwenden, die mit der parallelen Ressourcenverteilung eines Clusters funktioniert.\cite{noauthor_what_nodate} 
\par
Bei der horizontalen Skalierung (''scaling out'') werden einem Cluster zur Leistungssteigerung zusätzliche Nodes hinzugefügt. So kann ein Rechner zum Beispiel 500MB/s von seiner Festplatte lesen und verarbeiten, zehn Rechner lesen und verarbeiten in dieser Zeit allerdings 5000MB/s und können ihre Teilergebnisse anschließend zu einer Antwort zusammenfügen. Hierbei entsteht zwar zusätzlicher Netzwerk- und Verwaltungsaufwand (''Overhead''), aber die Leistungsfähigkeit des Clusters wächst mit jedem hinzugefügten Node. Ein horizontal skalierbares System ist mit höheren anfänglichen Kosten verbunden, kann dann aber bei linearem Kostenaufwand praktisch unendlich skaliert werden.\cite{noauthor_horizontal_nodate}\\
Wie im eingänglichen Zitat erwähnt, setzt Hadoop auf eben dieses Prinzip der Skalierbarkeit, um ``die über Computercluster verteilte Verarbeitung großer Datensätze mit einfachen Programmiermodellen''\cite{noauthor_apache_nodate} als Dienst mit hoher Verfügbarkeit anzubieten. Die Technologien, die konkret dahinter stecken, werden im nächsten Abschnitt behandelt.

\subsection{Historie}
2002 begannen Doug Cutting und Mike Cafarella ihre Arbeiten an Apache Nutch\footnote{https://nutch.apache.org/}, einer Open Source Web Search Engine als Teil des Apache Lucene Projekts\footnote{https://lucene.apache.org/}. Die beiden mussten Wege finden, um ihr Projekt auf die Milliarden Webseiten des Internets zu skalieren. 2003 veröffentlichte Google ein Whitepaper zur Architektur des Google File System (GFS), Googles eigenem verteilten Dateisystem.\footcite[The Google File System]{ghemawat_google_2003} Als Google 2004 dann ein weiteres Whitepaper zum MapReduce Programmiermodell veröffentlichte\footcite[MapReduce: Simplified Data Processing on Large Clusters]{dean_mapreduce_2004}, sahen Cutting und Cafarella darin die Lösung für Nutch's Skalierungsproblem. Sie implementierten eigene Versionen von MapReduce als Processing Engine und des GFS zur Datenhaltung (NDFS, Nutch Distributed File System) als Basis für Nutch. Da diese beiden Komponenten mannigfaltige Anwendungsfälle außerhalb der Web-Suche bedienen konnten, wurden sie 2006 als eigenes Projekt Apache Lucene unterstellt und erhielten den Namen \textbf{Hadoop}. Ungefähr zur gleichen Zeit wurde Doug Cutting von Yahoo! rekrutiert, um Hadoop dort mit zusätzlichen Ressourcen weiterzuentwickeln. 2008 wurde Hadoop schließlich zu einem Top Level Projekt der Apache Software Foundation.\footnote{https://hadoop.apache.org/}\cite{cutting_next_2016}

\begin{comment}
\section{Hadoop Versionsverlauf}
\subsection{Hadoop 1.x}
Das NDFS wurde zum Hadoop Distributed File System (HDFS) weiterentwickelt.
MapReduce war die einzige Processing Engine -> Nur große Java Applications
Scheduler der Single Point of Failure und Bottleneck

\subsection{Hadoop 2.x}
YARN als neue Ressourcenverwaltung
Haddop auf Windows
Bottleneck behoben, aber auch Möglichkeit geschaffen, ganz andere Prozesse im Cluster laufen zu lassen
In dem Fall muss man Dateien selbst aufteilen

\subsection{Hadoop 3.x}
Erasure Coding in HDFS
Support for Microsoft Azure Data Lake and Aliyun Object Storage System filesystem connectors
YARN Resource Types
\end{comment}

\subimport{Sections/}{Hadoop Architektur.tex}

