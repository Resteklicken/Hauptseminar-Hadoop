\chapter{Fazit}
Apache Hadoop ermöglicht auch für heutige Verhältnisse noch die Speicherung riesiger Datenmengen im HDFS, die Ausführung von Applikationen durch YARN und die parallele Verarbeitung von Datensätzen im Terabyte-Bereich durch MapReduce. Dabei bietet es einfache Skalierbarkeit, hohe Verfügbarkeit und große Zuverlässigkeit durch den verteilten Ansatz im Cluster. Man kann mit Hadoop die Vorteile der horizontalen Skalierung nutzen: linearen Kostenzuwachs und nahezu grenzenlose Skalierbarkeit. Durch das reichhaltige Ökosystem aus Frameworks und Tools, die sich um Hadoop herum entwickelt haben, kann es viele Anforderungen moderner Big Data Analytics erfüllen.
\par
Dabei kann man allerdings nicht verleugnen, dass manche Aspekte, wie die Auslagerung von Zwischenergebnis auf die Festplatte, teilweise ihre technische Relevanz verloren haben und Hadoop MapReduce als Processing Engine an Bedeutung verliert. Besonders Apache Spark kann viele Anwendungsfälle von MapReduce abdecken und bietet dabei einfachere Interfaces und mehrere Programmiersprachen. Geht es um Performance, Machine Learning und Echtzeitdatenverarbeitung, ist es Hadoop um Längen voraus. Dafür erfordert es den Einsatz teurerer Hardware, da es mehr Arbeitsspeicher benötigt.\footnote{Für eine Gegenüberstellung siehe https://www.ibm.com/cloud/blog/hadoop-vs-spark}
Hadoop ist auch lange nicht mehr das einzige Big Data Tool auf dem Markt. Die großen Cloudanbieter wie Google, AWS und Microsoft bieten hauseigene Big Data-Plattformen an, die wenig oder gar nicht auf Hadoop setzen.\footnote{https://cloud.google.com/solutions/smart-analytics}\footnote{https://aws.amazon.com/de/big-data/use-cases/}\footnote{https://azure.microsoft.com/de-de/solutions/big-data/} Dem entgegenzusetzen hat Hadoop seinen Status als Open-Source-Projekt.
\par
Schlussendlich kann man nicht sagen, dass Hadoop dabei ist, vollständig aus der Big Data-Landschaft zu verschwinden. Es bietet die Möglichkeit, Cluster mit skalierbaren Open Source-Lösungen zu relativ geringen Kosten zu betreiben. Ist man bereit, eventuell nicht die einfachsten APIs zu benutzen und auch mal ein paar Stunden auf seine Ergebnisse zu warten, dann findet man im Hadoop Ecosystem genug Tools für eine starke Big Data-Plattform. Besonders als Archivspeicher (\textit{Cold Storage}) eignet sich Hadoop, wenn man hin und wieder vorhat, Analysen auf großen Teilen seiner archivierten Daten durchzuführen. Will man hingegen vor allem explorative Datenanalysen, Machine Learning und Echtzeitdatenverarbeitung durchführen, sollte man nicht zu schnell in einen Hadoop Cluster investieren und sich erst die anderen Technologien auf dem Markt anschauen.