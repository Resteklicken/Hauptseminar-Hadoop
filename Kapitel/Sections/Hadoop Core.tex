\section{Hadoop Core}
Der Kern von Hadoop (Hadoop Core) besteht seit Hadoop 2.x aus vier Modulen, welche im offiziellen Download zusammengefasst sind\cite{noauthor_apache_nodate}:
\begin{itemize}
    \item Hadoop Distributed File System (HDFS™): Hadoops verteiltes Dateisystem
    \item Hadoop MapReduce: Hadoops Parallel Processing Engine für große Datenmengen
    \item Hadoop YARN: Ein Framework für Job Scheduling und Ressourcenverwaltung im Cluster
    \item Hadoop Common: Unterstützende Programme für die anderen Hadoop-Module
\end{itemize}
Diese Komponenten bringen alles mit, was man zur verteilten Verarbeitung und Speicherung großer Datenmengen benötigt. Dazu schreibt man in der Regel Java-Applikationen, die bestimmte Klassen aus den Bibliotheken von Hadoop ableiten. Beispiele zur Java-API und zur Streaming-API von MapReduce werden im Abschnitt \ref{chap:fund sec:core sub:handson mapred} gezeigt. 

\subsection{HDFS}
\label{chap:fund sec:core sub:hdfs}
Das HDFS ist ein Dateisystem, welches dem Anwender eine Abstraktionsschicht über verteilt gespeicherte Daten bietet. Dateien lassen sich ganz normal über einen Dateipfad im HDFS ansprechen, auch wenn sie im Hintergrund in Einzelteilen über viele Nodes verteilt gespeichert sind. Das HDFS ist für den Betrieb auf Clustern aus sogenannter \textit{Commodity Hardware} konzipiert. Commodity Hardware ist günstige, leicht zu ersetzende Hardware. Bei Commodity-Hardware-Clustern wird nicht etwa versucht, Ausfälle einzelner Nodes durch den Einsatz von besonders ausfallsicherer (und somit teurer) Hardware zu verhindern. Fällt ein Node aus, was in einem Cluster von hunderten Maschinen kein Sonderfall ist, übernimmt ein anderer Node dessen Arbeit, ohne dass dadurch die Verfügbarkeit des Clusters beeinträchtigt wird. Das HDFS setzt dafür auf die Konzepte von Blöcken, Replikation und Redundanz.\cite{white_hadoop_2015}  
\par
Ein vollwertiger Hadoop Cluster (Hadoop im \textit{fully-distributed Mode}) besteht aus mindestens einem Master, dem \textbf{NameNode}, und einem oder mehr Workern, den \textbf{DataNodes} (\textit{vgl. Abb. }\ref{fig:hdfs}). Um Dateien im HDFS zu speichern (Beispiel siehe \ref{chap:fund sec:core sub:handson hdfs}), teilt ein \textit{Client}-Prozess die Dateien in Blöcke von standardmäßig 128MB auf und kontaktiert den NameNode. Der NameNode hat einen Überblick über den verfügbaren Speicherplatz aller DataNodes und designiert manche davon, um einige der Blöcke aufzunehmen. Der NameNode achtet außerdem darauf, dass jeder einzelne Block repliziert und auf unterschiedlichen DataNodes gespeichert wird. Standardmäßig verteilt Hadoop drei Kopien eines jeden Blocks im Cluster, was durch den \textbf{Replication Factor} konfiguriert werden kann. Dadurch verbraucht man zwar drei mal so viel Speicher wie bei herkömmlichen, nicht redundanten Dateisystemen, erreicht dafür aber eine sehr hohe Verfügbarkeit. Der Einsatz von Commodity Hardware hält trotz des erhöhten Speicherbedarfs die Kosten niedrig.\cite{white_hadoop_2015}     
\par
Die DataNodes senden in regelmäßigen Abständen sogenannte \textit{Block Reports} an den NameNode. Dieser gleicht die Block Reports mit dem Soll-Zustand des Dateisystems ab. Ist zum Beispiel in einem Node eine Festplatte ausgefallen, so sind manche Blöcke unterrepliziert. Der NameNode veranlasst DataNodes, die Kopien der betroffenen Blöcke besitzen dazu, diese an andere DataNodes zu senden, bis der Soll-Zustand des Clusters wieder hergestellt ist.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{hdfsarchitecture}
    \caption[Architektur des HDFS]{Architektur des HDFS\parencite[S.69, Figure 3-2]{white_hadoop_2015}}
    \label{fig:hdfs}
\end{figure}

\subsection{MapReduce}
\label{chap:fund sec:core sub:mapred}
MapReduce heißt sowohl ein Programmiermodell zur parallelisierten Verarbeitung von Datensätzen, als auch die konkrete Implementierung eben dieses Modells als Komponente des Hadoop Frameworks. MapReduce macht sich mehrere Prinzipien zu Nutze, um effizient mit großen Datenmengen umzugehen\cite{freiknecht_big_2018}: \\
\textbf{Aufteilung}: Eingabedaten werden in \textbf{InputSplits} geteilt verarbeitet. Dadurch verarbeitet ein einzelner Prozess ein logisch zusammenhängendes Datenpaket.\\
\textbf{Parallelisierung}: InputSplits werden parallel auf mehreren Nodes bearbeitet und die Ausgaben zusammengeführt. Dadurch werden auch bei großen Datenmengen hohe Datendurchsatzraten erreicht.\\
\textbf{Datenlokalität}: Der erste Teil der Verarbeitungslogik, die Mapping-Phase, wird möglichst nahe an den Daten durchgeführt; wenn möglich auf den Nodes, auf denen die Daten gespeichert sind. Ansonsten wird versucht, die Verarbeitung wenigstens auf dem gleichen Server Rack durchzuführen, um die Belastung der Netzwerkinfrastruktur so gering wie möglich zu halten.
\par
Ein MapReduce Job besteht aus zwei Phasen: der \textbf{Map-Phase} und der \textbf{Reduce-Phase}. Logisch kann man dazwischen noch die \textbf{Sort- und Shuffle-Phase} unterscheiden (siehe Abb. \ref*{fig:mapred}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{MapReduce}
    \caption[Die Phasen von MapReduce]{Die Phasen von MapReduce\parencite[Seite 34, Figure 2-4]{white_hadoop_2015}}
    \label{fig:mapred}
\end{figure}
Wie eingangs erwähnt, wird die Eingabe in InputSplits zerteilt. Diese werden wiederum in einzelne Datensätze, die \textbf{Records}, aufgespalten. Wie diese Aufteilung abläuft, wird durch das \textbf{InputFormat} bestimmt, welches vom Anwender im Programmcode festgelegt und auf das Format der Eingabedaten abgestimmt werden muss. Dabei stehen zum Beispiel \textit{TextInputFormat} oder \textit{KeyValueTextInputFormat} zur Verfügung. Es ist auch möglich, durch Ableiten der abstrakten Java Klasse \textit{InputFormat} eigene InputFormats zu schreiben.\cite{white_hadoop_2015}
\par
Für jeden InputSplit wird ein eigener Map-Prozess (\textbf{Mapper}) gestartet. Dieser erhält alle Records des InputSplits in Form von \textbf{Key-Value-Paaren} als Eingabe. Auf jeden Record wird eine vom Anwender geschriebene Map-Funktion angewendet, die oftmals die Daten filtert und vorbereitet, zum Beispiel durch Parsen von Strings in Integer. Die Daten werden vom Eingabeformat in bereinigte Key-Value-Paare \textbf{gemappt}. Das Ergebnis wird an Reduce-Prozesse (\textbf{Reducer}) weitergegeben. Ein Beispiel dazu wird in Abschnitt \ref{chap:fund sec:core sub:handson mapred} besprochen.
\par
Bevor die Key-Value-Paare an die Reducer gegeben werden, werden sie nach Keys sortiert und gruppiert. Dies geschieht in der Sort- und Shuffle-Phase. Der Input für den Reducer ist dann eine Liste mit Key-Value-Paaren, wobei die Values wiederum Listen mit den Werten sind, die von den Mappern für den jeweiligen Key gefunden wurden (vgl. Abb. \ref{fig:mapred dataflow}). 
\par
Der Reducer wendet eine ebenfalls vom Anwender geschriebene Reduce-Funktion auf die ihm übergebenen Daten an. Für jeden Key wird die Liste aus Values zu einem einzigen Value \textbf{reduziert}, zum Beispiel durch Bestimmung des Maximums oder Aufsummierung aller Teilwerte. Die Anzahl der Reduce-Prozesse bestimmt die Anzahl der Ausgabedateien (eine Datei pro Reducer) und \textit{kann} im Programmcode festgelegt werden. Allerdings sollte man gute Gründe haben, um die von Hadoop gewählten Werte zu überschreiben, da dies katastrophale Folgen für die Performance haben kann.\cite{infrabot_howmanymapsandreduces_2019} 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{MapReduce Dataflow}
    \caption[Der MapReduce Dataflow]{Der MapReduce Dataflow\parencite[Seite 24, Figure 2-1]{white_hadoop_2015}}
    \label{fig:mapred dataflow}
\end{figure}

MapReduce hat immer noch eine Sonderstellung im Hadoop Ecosystem, da es die mitgelieferte Processing Engine ist. Vor Hadoop 2.x war es sogar die einzige Möglichkeit, Daten in einem Hadoop-Cluster zu verarbeiten. Eine MapReduce-Applikation zu entwickeln erfordert allerdings das Schreiben vielen Java-Codes und Problemstellungen müssen in die Phasen von Mapping und Reducing übertragen werden, selbst wenn andere Modellierungen der Problemstellung intuitiver oder leichter zu bearbeiten wären. Außerdem ist MapReduce allein für \textbf{Batch Processing} ausgelegt. Das heißt, dass man eine MapReduce-Applikation schreibt, die eine ganz bestimmte Fragestellung zu einem Datensatz beantwortet. Diese wird ausgeführt und erst nachdem alle Daten verarbeitet wurden, sieht man ein Ergebnis. Dies kann viele Minuten, Stunden oder sogar Tage dauern. Will man nun einen Parameter der Fragestellung ändern (zum Beispiel nicht mehr nach Monaten sondern nach Wochen aufgeschlüsselt), muss man die gesamte Verarbeitung des Datensatzes noch einmal durchführen. Dies steht im Konflikt mit der heute üblichen Forderung nach \textbf{visueller Datenexploration}\cite{keim_datenvisualisierung_nodate}. Zuguterletzt ist MapReduce nach heutigen Standards eher langsam. Da es für die Ausführung auf Commodity Hardware entwickelt wurde, schreibt und liest es die Zwischenergebnisse der einzelnen Phasen immer wieder von der Festplatte des DataNodes. Neue Processing Engines (allen voran Apache Spark \footnote{https://spark.apache.org/}), setzen viel auf \textbf{In-Memory Processing}, halten alle Daten also möglichst während der gesamten Bearbeitungszeit im Arbeitspeicher. Das ermöglicht bis zu 40 mal schnellere Abfragen bei gleichwertigem Arbeitsaufwand. \cite[vgl.][Kap. 3.19]{freiknecht_big_2018} 

\subsection{YARN}
\label{chap:fund sec:core sub:yarn}
In Version 1.x von Hadoop war MapReduce sowohl für die Verarbeitung der Daten, als auch für die Ressourcenzuteilung im Cluster zuständig. Das bedeutete, dass man zwingend das MapReduce-Programmiermodell nutzen musste, um die im Hadoop Cluster gespeicherten Daten auszuwerten. Die Ressourcenverwaltung war damit ein mögliches Bottleneck, da sie bei mehreren parallel laufenden Jobs auf einem Node um Rechenzeit mit der Datenverarbeitung konkurrieren musste und neue Jobs gegebenfalls lange nicht gestartet wurden.\cite{freiknecht_big_2018}
Die größte Änderung in Hadoop 2.x war dann die Ausgliederung der Ressourcenverwaltung aus MapReduce und die Einführung einer dedizierten Ressourcenverwaltungsanwendung - \textbf{YARN} - \textit{Yet Another Ressource Negotiator}. YARN teilt eingehenden Jobs Cluster-Ressourcen zu und startet fehlgeschlagene Jobs gegebenenfalls neu. Ähnlich wie das HDFS bringt YARN eine Reihe von Prozessen mit sich, die Master- und Worker-Rollen einnehmen. Auf dem vom HDFS designierten NameNode läuft der \textbf{Resource Manager}. Dieser unterteilt sich wiederum in \textbf{Application Manager} und \textbf{Scheduler}. Auf allen DataNodes läuft jeweils ein \textbf{Node Manager}.\cite{freiknecht_big_2018} 
\par
Durch das Zusammenspiel dieser Prozesse bietet sich dem Anwender ein Interface zur verteilten Ausführung von Anwendungslogik, bei dem man sich nicht an das MapReduce-Programmiermodell halten muss. Startet man in Hadoop 2.x eine MapReduce-Applikation, ist diese eigentlich eine YARN-Applikation, bei der einem schon ein Teil des Programmieraufwands abgenommen wurde. Eine eigene YARN-Applikation zu schreiben bedeutet hingegen, sich selbst um die logische Aufteilung der Daten zu kümmern, Cluster-Ressourcen wie CPU und RAM in Form sogenannter \textbf{Container} von YARN anzufordern und dafür zu sorgen, dass der auszuführende Programmcode für alle DataNodes (am besten gespeichert im HDFS) verfügbar ist. YARN reiht die Ausführung der angeforderten Container auf verschiedenen DataNodes in Warteschlangen ein, kopiert den Anwendungscode aus dem HDFS auf diese Nodes und überwacht die erfolgreiche Ausführung der Anwendung. 
  
\subsection{Umgang mit HDFS und MapReduce}
Zur Installation von Hadoop kann man die offizielle Distribution\footnote{https://hadoop.apache.org/releases.html} benutzen und komplett selbst konfigurieren. Dabei besteht die Möglichkeit, Hadoop in drei verschiedenen Modi zu betreiben: \textbf{Single Node}, \textbf{Pseudo-distributed} und \textbf{Fully-distributed}. Erstere beide sind zum Testen und Entwickeln, letztere für den tatsächlichen Einsatz im Cluster gedacht\cite[vgl.][Kap. 3.4]{freiknecht_big_2018}.
Weiterhin haben diverse kommerzielle Anbieter wie Cloudera\footnote{https://de.cloudera.com/} eigene Hadoop Distributionen entwickelt, die sie in Form von vorkonfigurierten VM- oder Docker-Images teilweise kostenlos zur Verfügung stellen. Cloudera zum Beispiel ergänzt diese Distributionen aber mittlerweile durch Cloudlösungen\footnote{https://de.cloudera.com/products/cloudera-data-platform.html}.   
Cloudanbieter wie Amazon, Google und Microsoft bieten fertig konfigurierte und voll verwaltete Cluster auf ihren jeweiligen Cloudplattformen an (Amazon EMR\footnote{https://aws.amazon.com/emr/features/hadoop/}, Google Dataproc\footnote{https://cloud.google.com/dataproc} und Azure HDInsight\footnote{https://azure.microsoft.com/en-us/products/hdinsight/\#overview}). 

\subsubsection*{Single Node Setup}
Hadoop bietet unzählige Einstellungsmöglichkeiten während der Installation und sie alle zu behandeln würde den Rahmen dieser Arbeit sprengen. Daher wird in diesem Abschnitt ein VirtualBox Image von Cloudera (die Hortonworks Data Platform (HDP) Sandbox)\footnote{Download: https://www.cloudera.com/downloads/hortonworks-sandbox/hdp.html} genutzt. Dieses kann auf dem eigenen Rechner oder auf einem Remote-Host gestartet werden\footnote{Installation: https://www.cloudera.com/tutorials/sandbox-deployment-and-install-guide.html} und bietet Zugriff auf eine voll konfigurierte Installation von Hadoop im Single Node Modus. Zusätzlich sind noch ergänzende Komponenten aus dem Hadoop Ecosystem installiert, auf die in späteren Kapiteln eingegangen wird.

\subsubsection*{Hadoop und Ambari in der HDP Sandbox VM}
Im Single Node Modus laufen alle Hadoop-Prozesse auf \textit{einem} Host(sprich Rechner). Dieser Modus ist für die Entwicklung und zum Testen von Hadoop gedacht, da es keinen praktischen Nutzen bringt, ein Framework zur verteilten Datenverarbeitung ohne die entsprechende Verteilung über einen Cluster zu betreiben. Auch wenn in dieser VM nur ein Cluster bestehend aus einem Node aufgesetzt wurde, steht das Cluster-Verwaltungs-Tool \textbf{Apache Ambari}\footnote{https://ambari.apache.org/} zur Verfügung. Normalerweise ist das Aufsetzen eines Hadoop Clusters mit dem Bearbeiten vieler XML-Konfigurationsdateien und der Ausführung von Start-Bash-Skripten auf allen Nodes verbunden. Auf diese Weise bestimmt man, welcher Node der NameNode des HDFS werden soll, welche Nodes DataNodes werden, wo der ResourceManager von YARN läuft und so weiter. Ambari bietet einem für all das (und noch viel mehr) eine übersichtliche Weboberfläche. Diese erreicht man nach Starten der HDP Sandbox unter \verb|http://localhost:8080|. Die Rechte und Anwendungsszenarien der verschiedenen Nutzeraccounts, eine Anleitung zum (Zurück)setzen des Admin-Passworts und weitere Schrite nach der Installation findet man auf Clouderas Hilfeseite zur Sandbox\footnote{https://www.cloudera.com/tutorials/learning-the-ropes-of-the-hdp-sandbox.html}. Loggt man sich als 'admin' zeitnah nach dem Start der VM in das das Ambari Dashboard ein, kann man im Header des Dashboards auf das Zahnrad klicken und den Prozess in Arbeit sehen, der die Dienste aller Hadoop Komponenten startet (siehe Abb. \ref{fig:cluster startup}). 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{cluster_startup}
    \caption[Start aller Prozesse beim Cluster Startup]{Start aller Dienste beim Cluster Startup}
    \label{fig:cluster startup}
\end{figure}

Ist der Prozess abgeschlossen, kann man im Navigationsmenü auf der linken Seite unter dem Eintrag \textit{Hosts} eine Auflistung aller Nodes im Cluster (hier nur ein einziger) sehen. Durch Anklicken des Hostnamens gelangt man in die Host-Übersicht, wo man unter anderem die Liste der laufenden Komponenten findet. Abbildung \ref{fig:hdp core processes} zeigt ein Bild davon. Der Übersicht halber wurden nur die Hadoop Core Komponenten abgebildet. Wie man sehen kann, laufen auf diesem Host alle Master-, Worker-, und Client-Prozesse gleichzeitig. Man findet die in Abschnitt \ref{chap:fund sec:core sub:yarn} angesprochenen YARN-Komponenten ResourceManager und NodeManager, die HDFS-Komponenten DataNode und NameNode aus Abschnitt \ref{chap:fund sec:core sub:hdfs}, sowie die Client-Dienste für YARN, HDFS und MapReduce wieder. 

\begin{figure}[ht]
    \centering
    \includegraphics[width= 0.6\textwidth]{hdp_core_processes}
    \caption[Hadoop Core Prozesse in der HDP Sandbox]{Hadoop Core Prozesse in der HDP Sandbox}
    \label{fig:hdp core processes}
\end{figure}

Im Tab \textit{Configs} kann man die sonst über viele XML-Dateien verstreuten Einstellungen der Komponenten vornehmen. So kann man zum Beispiel unter \textit{HDFS -> Advanced -> General -> Block replication} (siehe Abb. \ref{fig:block replication factor}) den Replication Factor des HDFS verändern. Dieser ist in der Sandbox auf \verb|1| gestellt, da es im Single Node Modus nur einen Node gibt und das Speichern mehrerer Block-Replika auf dem gleichen Node keinen Vorteil bringt.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{block_repl_factor}
    \caption[Replication Factor Einstellung in Ambari]{Replication Factor Einstellung in Ambari}
    \label{fig:block replication factor}
\end{figure}


\subsubsection*{Fallstudie Globales Wetter}
Als Fallstudie für die Nutzung von Hadoop werden die täglichen Zusammenfassungen aller Wetterstationen der Welt aus dem Katalog der National Centers for Environmental Information\footnote{https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-day} benutzt. Dieses Beispiel wurde größtenteils entnommen aus \citefield[\textit{(S. 19-30, 693-695)}]{white_hadoop_2015}{title}. Da durch das lokale Setup die Kapazitäten von Hadoop auf die des Host-Computers beschränkt sind, werden hier nur die Datensätze der Jahre 2016 bis 2022 in komprimierten Archiven heruntergeladen\footnote{Download: https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/}. Diese enthalten jeweils die gesammelten Aufzeichnungen eines Jahres in Form von CSV-Dateien mit geringer Dateigröße (siehe Abb. \ref{fig:ncdc files local}). Das HDFS zeigt jedoch seine Stärken erst bei Datensätzen im Gigabyte- bis Terabyte-Bereich und kann nicht effizient mit vielen kleinen Dateien umgehen\footcite[vgl.][Assumptions and Goals -> Large Data Sets]{noauthor_apache_nodate-1}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{ncdc_files}
    \includegraphics[width=0.7\textwidth]{ncdc_archive_content}
    \caption[NCDC Archive für die Fallstudie]{NCDC Archive für die Fallstudie}
    \label{fig:ncdc files local}
\end{figure}

Das HDFS und MapReduce arbeiten außerdem mit wenigen großen Dateien wesentlich besser als mit vielen kleinen Dateien.\cite[\textit{(S. 19-30, 693-695)}]{white_hadoop_2015} Deshalb wird der erste Schritt sein, die Daten in das HDFS zu laden. Anschließend wird eine MapReduce-Applikation mit der MapReduce Streaming-API geschrieben. Diese wird nur aus einer Map-Phase bestehen und die Dateien säubern und zusammenzuführen. Zuguterletzt wird eine weitere MapReduce-Applikation mit der Java-API geschrieben, die die Daten auswertet. In späteren Abschnitten werden weitere Komponenten des Hadoop Ecosystems vorgestellt, mit denen die Dateien weiter verarbeitet werden.
\subsubsection*{Umgang mit dem HDFS}
\label{chap:fund sec:core sub:handson hdfs}
Es gibt verschiedene Wege, um Dateien ins HDFS zu laden\footnote{3 Beispiele: https://community.cloudera.com/t5/Support-Questions/Import-data-from-remote-server-to-HDFS/m-p/233149/highlight/true\#M194979}. In der Sandbox wurde ein Ambari View, der \textbf{Files View}\footnote{https://docs.cloudera.com/HDPDocuments/Ambari-2.7.4.0/using-ambari-views/content/\\amb\_using\_files\_view.html}, erstellt. Diesen erreicht man über das Kachel-Icon im Header des Dashboards\footnote{http://<localhost oder Sandbox Hostname>:8080/\#/main/view/FILES/auto\_files\_instance}. Der Files View bietet einen HDFS Dateibrowser mit Funktionalitäten wie Datei-Upload direkt von der lokalen Maschine, Ordnererstellung und Rechteverwaltung. Nicht alle Nutzer der Sandbox haben die nötigen Rechte zur Ausführung der Befehle. Daher sollte man sich mit dem dafür vorgesehenen Nutzer\footnote{https://www.cloudera.com/tutorials/learning-the-ropes-of-the-hdp-sandbox.html\#login-credentials} \verb|user: maria_dev| \verb|password: maria_dev| in Ambari einloggen. Man kann über den Files View aber nur eine Datei zur Zeit hochladen. Das Hochladen von Archiven (gestestet mit \verb|.tar.gz| und \verb|.jar|) schließt ebenfalls nie ab. Da für den nachfolgenden Schritt ein JAR aus dem HDFS als Input eingebunden werden soll, wird ein HDFS Client Prozess verwendet, der mit dem Hadoop Cluster verbunden ist. Dafür ist es nötig, Hadoop lokal installiert zu haben, oder man kopiert erst per \verb|scp| die Dateien auf das lokale Dateisystem eines Nodes im Cluster, auf dem ein HDFS Client läuft. Hierfür verwendet man zum Beispiel den NameNode. 
\par
\textbf{Schritt 1} ist das Erstellen des JARs \verb|ncdc.jar| mit den Eingabedateien des NCDC (siehe Abb. \ref{fig:ncdc jar local}). Hadoop wird dieses nachher dem MapReduce Job bereitstellen und automatisch entpacken. \textbf{Schritt 2} ist das Erstellen der Textdatei \verb|file_names.txt| mit den Dateipfaden der Unterordner des JARs (siehe Abb. \ref{fig:file names txt local}). Diese Datei wird der Input des MapReduce Jobs. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{ncdc_jar_local}
    \caption[Erstellung des JARs mit Wetterdaten]{Erstellung des JARs mit Wetterdaten}
    \label{fig:ncdc jar local}
    \includegraphics[width=0.75\textwidth]{file_name_list}
    \caption[Inputdatei des MapReduce Jobs]{Inputdatei des MapReduce Jobs}
    \label{fig:file names txt local}
\end{figure}

In \textbf{Schritt 3} gilt es, die bisher erstellten Dateien ins HDFS zu übertragen. Dazu muss man sich per \verb|ssh| als Benutzer \verb|maria_dev| mit der Sandbox verbinden. Um die Ordnerstruktur im HDFS zu erstellen, nutzt man entweder den Ambari File View, oder den Befehl \verb|hdfs dfs -mkdir -p /user/maria_dev/input|. \verb|hdfs| ist das Programm zur Interaktion mit dem Hadoop Distributed File System. \verb|dfs| startet die \textbf{Filesystem Shell}, mit der man eine Reihe von ''shell-artigen'' Kommandos\footnote{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html} direkt auf dem HDFS anwenden kann. An dieser Stelle könnte zum Beispiel auch \verb|dfsadmin| stehen, wenn es um die Administration des Dateisystems als solches ginge (einen DataNode abschalten etc.). FS Shell Commands beginnen mit einem Bindestrich, darauf folgt ein Befehl und dahinter kommen die Parameter. \verb|-mkdir -p /user/maria_dev/input| funktoniert genau wie sein Gegenstück aus einer normalen Shell. Der Ordner \verb|input| sollte noch für alle Benutzer als schreibbar markiert werden, da dort hin später die zusammengefassten CSV-Dateien geschrieben werden sollen. Dazu dient der Befehl \verb|hdfs dfs -chmod 777 /user/maria_dev/input|. Ist die Ordnerstruktur erstellt, kann man von seiner lokalen Maschine mit \verb|scp -P <Port> <lokaler Pfad> maria_dev@<Hostname>:<Host Pfad>| die bisher erstellten Dateien (JAR und Textdatei) auf das lokale Dateisystem des NameNodes (der Sandbox) kopieren(Abb. \ref{fig:scp transfer}). Anschließend werden diese per FS Shell mit dem Befehl \verb|-moveFromLocal| in das HDFS geladen, damit die Dateien vom lokalen Dateisystem des NameNodes entfernt werden, nachdem sie im HDFS gespeichert wurden (Abb. \ref{fig:hdfs move}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{scp_ncdc_transfer}
    \caption[Dateitransfer mit scp]{Dateitransfer mit scp}
    \label{fig:scp transfer}
    \includegraphics[width=0.75\textwidth]{hdfs_ncdc_move}
    \caption[Dateiupload in das HDFS]{Dateiupload in das HDFS}
    \label{fig:hdfs move}
\end{figure}

\textbf{Schritt 4} ist das Schreiben eines Bash Skripts, welches später als Mapper-Klasse fungiert. Zum Zusammenführen der Dateien wird nicht die Java API von MapReduce, sondern die Streaming API von Hadoop\footnote{https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html} verwendet. Diese erlaubt es, völlig sprachenunabhängig MapReduce Jobs zu schreiben. Man kann als Mapper und/oder Reducer dabei jeweils eigene Skripte oder Binaries verwenden. Jeder Mapper-Prozess führt eine Instanz des angegebenen Mapper-Skripts aus und füttert ihm, sofern nicht ein anderes InputFormat bestimmt wurde, Zeile für Zeile den Inhalt des ihm zugeteilten InputSplits als Key-Value-Paare auf \verb|stdin|. \verb|stdout| des Skripts wird wiederum vom Mapper zeilenweise als Key-Value-Paar gesammelt und an die Reducer weitergeleitet, wo der Prozess genauso abläuft. Das Skript ist in Listing \ref{lst:ncdc concatenate script} bis auf einige Kommentare abgebildet. Das gesamte Skript findet sich im Appendix \ref{appendix lst:ncdc concatenate script}.    

\lstinputlisting[language=bash, caption={Bash Skript als Mapper-Klasse}, linerange={28-51}, label={lst:ncdc concatenate script}]{code/concatenate_ncdc_data.sh}

Mit \verb|read offset inputfile| liest das Skript das Key-Value-Paar von \verb|stdin| ein. Durch das später bei der Ausführung angegebene InputFormat \verb|NLineInputFormat| ist das erste Wort der Zeile der Offset der Zeile zum Dateianfang. Diese Information ist für diesen Job nicht relevant. Im Rest der Zeile steht der Inhalt der ürsprünglichen Zeile, in diesem Fall ein Dateiname wie \verb|ncdc.jar/20xx.tar.gz|. 
Das Archiv aus dem \verb|ncdc.jar| wird in einen Unterordner in der Laufzeitumgebung des MapReduce Jobs entpackt. Anschließend werden alle darin befindlichen CSV-Dateien zu einer Datei vereint. Diese resultierende Datei wird zum Schluss wieder komprimiert und mit dem Befehl \verb|hdfs dfs -put - <Pfad>| im HDFS abgespeichert. 
\par
\textbf{Schritt 5}:Ist das Skript geschrieben, könnte es wieder per \verb|scp| und \verb|hdfs dfs| in das HDFS übertragen werden. An dieser Stelle soll aber der Ambari File View gezeigt werden. Zu diesem navigiert man wie bereits in Abschnitt \ref{chap:fund sec:core sub:handson hdfs} beschrieben und wechselt über die grafische Oberfläche in den Zielordner \verb|/user/maria_dev/input|. Über den Upload Button oben rechts kann man die Datei direkt von seinem lokalen Rechner ins HDFS laden. Am Ende sollte der Inhalt des Ordners aussehen wie in Abbildung \ref{fig:ncdc files hdfs}. Das Skript muss noch für alle Nutzer als ausführbar markiert werden, damit der MapReduce Job es später benutzen kann. Dafür wählt man die Datei an, klickt oben links auf \verb|Permissions| und setzt sie entsprechend Abbildung \ref{fig:concat script permissions}. Man könnte hierfür auch das HDFS CLI mit dem Befehl \verb|hdfs dfs -chmod 755 <Pfad>| verwenden.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{ncdc_files_on_hdfs}
    \caption[Upload der NCDC Datensätze]{Upload der NCDC Datensätze}
    \label{fig:ncdc files hdfs}
    \includegraphics[width=0.5\textwidth]{script_permissions}
    \caption[Setzen von Dateiberechtigungen im Ambari File View]{Setzen von Dateiberechtigungen im Ambari File View}
    \label{fig:concat script permissions}
\end{figure}

\textbf{Schritt 6} besteht aus dem Zusammenstellen des Befehls zur Ausführung des MapReduce Jobs (siehe Listing \ref{lst:ncdc concatenate start script}). Der vollständig dokumentierte Befehl ist in Appendix \ref{appendix lst:ncdc concatenate start script} zu finden. 
\begin{itemize}
    \item Zeile 1 weist Hadoop an, ein JAR auszuführen. Dateipfad des Hadoop Streaming JARs angegeben, welches mit dem Framework mitgeliefert wird.
    \item Zeile 2 sorgt dafür, dass Hadoop das im HDFS gespeicherte JAR \verb|ncdc.jar| in die Laufzeitumgebung des MapReduce Jobs kopiert und automatisch entpackt. Prozesse können während des Jobs unter dem Pfad \verb|ncdc.jar| auf die darin enthaltenen Dateien (hier die TARs mit den jährlichen Datensätzen) zugreifen.
    \item Zeile 3 weist Hadoop eigentlich an, die Mapper-Klasse vom lokalen Dateisystem auf alle am Job beteiligten Nodes zu kopieren, damit sie dort lokal zur Verfügung steht. Durch das Präfix \verb|hdfs://host:port/| teilt man Hadoop mit, dass die Datei bereits im HDFS liegt. Mit \verb|\#concatenate\_ncdc\_data.sh| am Ende des Pfades gibt man der Datei einen Alias, damit man in der \verb|-mapper| Option nicht wieder den vollen Pfad angeben muss.      
    \item Mit \verb|-D| kriegt ein Parameter Priorität über eine Einstellung, die bereits in Konfigurationsdateien gesetzt sind.
    \item Zeile 4 macht aus diesem Job einen reinen Map-Job ohne Reduce-Phase, da für die Umwandlung der Dateien keine Reduce-Phase nötig ist.
    \item Zeile 5 verhindert die sogenannte spekulative Ausführung. Ist diese Option aktiviert, startet Hadoop manchmal mehrere Jobs für einen InputSplit und filtert in der Shuffle-Phase doppelte Ergebnisse. Sind manche Nodes deutlich langsamer als andere, kann das Performancegewinne bringen. In diesem Fall würde das aber dazu führen, dass Dateien doppelt ins HDFS geschrieben würden, da nicht der Output der Mapper verwendet wird, sondern das Skript direkt ins HDFS schreibt.
    \item Zeile 6 gibt den Pfad zur Datei im HDFS an. Anders als bei der Angabe des mitzuliefernden JARs muss bei den Parametern für Input und Output kein vollständiger HDFS-Pfad angegeben werden. MapReduce erwartet standardmäßig, dass es sich dabei um Verzeichnisse und Dateien im HDFS handelt.
    \item Zeile 7 gibt die Java Klasse des InputFormats an. Diese gehört zum Hadoop Framework und ist im Java Class Path der Ausführungsumgebung verfügbar. Mit \verb|NLineIn-| \verb|putFormat| werden einem Mapper \textit{N} Zeilen aus dem InputSplit als Eingabe gegeben. \textit{N} ist standardmäßig \verb|1|. 
    \item Zeile 8 gibt den Pfad an, unter dem die Reducer ihre Ausgabedateien ablegen werden. Es muss sich hierbei um einen Ordner handeln, der noch nicht existiert. Im HDFS können Dateien nicht einfach überschrieben werden. Daher dürfen Ausgabeordner grundsätzlich nicht vorher existieren.
    \item Zeile 9 gibt die Mapper-Klasse an. Würde die Java API statt der Streaming API verwendet, stünde hier eine Java Klasse. Es wird der Alias aus Zeile 3 benutzt.
\end{itemize}

\lstinputlisting[language=bash, linerange={7-15}, caption={Startskript für den NCDC Concatenation MapReduce Job}, label={lst:ncdc concatenate start script}]{code/concat_files_start.sh}

\textbf{Schritt 7} 
Sind alle Dateien an den richtigen Orten platziert, kann man sich erneut als \verb|maria_dev| per \verb|ssh| auf den NameNode verbinden und den Befehl aus Schritt 6 absetzen. Die Ausführung kann mehrere Minuten dauern. Am Ende sollte die Konsole eine ähnliche Ausgabe wie in Abbildung \ref{fig:ncdc concat console output} zeigen. Es wurden der Übersicht halber viele Zeilen in der Abbildung entfernt.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{ncdc_concat_output}
    \caption[MapReduce Konsolenausgabe]{MapReduce Konsolenausgabe}
    \label{fig:ncdc concat console output}
\end{figure}

\textbf{Schritt 8}
Betrachtung der Ergebnisse
\begin{comment}
Daher wird in diesem Beispiel stattdessen das \textbf{WebHDFS}\footnote{https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/WebHDFS.html} benutzt.
\par
Das WebHDFS ist in der Sandbox bereits vollständig eingerichtet. Mit dessen Hilfe kann man ohne lokale Hadoop Installation per REST API mit dem HDFS interagieren. Man kann zum Beispiel auf seiner lokalen Maschine den Befehl zum Auflisten aller Dateien im \verb|/tmp/| Verzeichnis des HDFS mittels \verb|curl| absetzen. Dieser lautet 
\begin{lstlisting}[breaklines]
    curl -i "http://<hostname>:<port>/webhdfs/v1/tmp/?op=LISTSTATUS"
\end{lstlisting}
\begin{itemize}
    \item Mit der Option \verb|-i| werden die Response Header angezeigt.
    \item \verb|hostname| kann \verb|localhost| oder ein in der \verb|/etc/hosts| festgelegter Hostname für die Sandbox sein.
    \item Der Port ist standardmäßig \verb|50070|.
    \item Alles zwischen \verb|/v1| und \verb|?op| ist der Dateipfad.
    \item Der Parameter \verb|LISTSTATUS| ist der auszuführende HDFS Befehl.    
\end{itemize}

    

Abbildung \ref{fig:tmp list} zeigt die Ausgabe. Die Antwort ist ähnlich der eines typischen \verb|ls -l| Befehls: Zu sehen sind Dateien, deren Typ (hier vier mal 'DIRECTORY'), Zugriffsrechte, etc. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{webhdfs_list_tmp}
    \caption[Dateiauflistung per WebHDFS]{Dateiauflistung per WebHDFS}
    \label{fig:tmp list}
\end{figure}
\end{comment}


\subsubsection*{Die MapReduce Java API}
\label{chap:fund sec:core sub:handson mapred}

\begin{comment}
    \subsubsection*{Fully-distributed Cluster}
Installation von Hadoop auf allen beteiligten Maschinen
Einrichtung von passwordless ssh auf allen Maschinen
Evtl. Anpassung der /etc/hosts auf allen Maschinen
Editieren der ganzen Konfigurationsdateien (XML) und kopieren der gleichen Dateien auf alle beteiligten Maschinen
Editieren der Worker Datei auf dem NameNode
NameNode (HDFS) formatieren
Ausführen der Skripte auf dem NameNode

\subsubsection*{Hadoop in der Cloud}
Dabei kann man statt des HDFS zur Datenhaltung die jeweiligen Cloud Storage Systeme (Google Cloud Storage\footnote{https://cloud.google.com/blog/products/storage-data-transfer/hdfs-vs-cloud-storage-pros-cons-and-migration-tips} und Azure Storage / Azure Data Lake Storage\footnote{https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-architecture}) nutzen.
Grafische Oberfläche zum Submitten von Jobs, etc.

\end{comment}


